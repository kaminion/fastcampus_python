{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 규제 (Regularization)\n",
    "\n",
    "학습이 과대적합 되는 것을 방지하고자 일종의 penalty를 부여하는 것\n",
    "\n",
    "##### L2 규제(L2 Regularization)\n",
    "- 각 가중치 제곱의 합에 규제 강도(Regularization Strength) 람다를 곱한다.\n",
    "- 람다를 크게 하면 가중치가 더 많이 감소되고(규제를 중요시 함), 람다를 작게 하면 가중치가 증가한다(규제를 중요시하지않음)\n",
    "\n",
    "##### L1 규제(L1 Regularization)\n",
    "\n",
    "\n",
    "**L2 규제가 L1 규제에 비해 더 안정적이라 일반적으로는 L2 규제가 더 많이 사용된다.**\n",
    "\n",
    "L1 규제에서 가중치가 0이 되는것 때문에..\n",
    "\n",
    "\n",
    "Y = w * X + bias\n",
    "(예측값) = 가중치 * 인풋데이터 + 바이아스 값(편견)\n",
    "예측에선 가중치와 바이아스가 계속 업데이트 되면서 조정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 규제를 활용한 모델 \n",
    "\n",
    "릿지(Ridge) - L2규제 \n",
    "Error = MSE + aw^2\n",
    "\n",
    "라쏘(Lasso) - L1 규제\n",
    "Error = MSE + a|w|"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}